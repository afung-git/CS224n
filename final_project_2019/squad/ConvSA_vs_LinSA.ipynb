{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Initialized_Conv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,\n",
    "                 kernel_size=1, stride=1, padding=0, groups=1,\n",
    "                 relu=False, bias=False):\n",
    "        super().__init__()\n",
    "        self.out = nn.Conv1d(\n",
    "            in_channels, out_channels,\n",
    "            kernel_size, stride=stride,\n",
    "            padding=padding, groups=groups, bias=bias)\n",
    "        nn.init.constant_(self.out.weight, 1.)\n",
    "        if relu is True:\n",
    "            self.relu = True\n",
    "#             nn.init.kaiming_normal_(self.out.weight, nonlinearity='relu')\n",
    "        else:\n",
    "            self.relu = False\n",
    "#             nn.init.xavier_uniform_(self.out.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.relu is True:\n",
    "            return F.relu(self.out(x))\n",
    "        else:\n",
    "            return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_logits(target, mask):\n",
    "    mask = mask.type(torch.float32)\n",
    "    return target * mask + (1 - mask) * (-1e30)  # !!!!!!!!!!!!!!!  do we need * mask after target?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_head, dropout):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_head = num_head\n",
    "        self.dropout = dropout\n",
    "        self.mem_conv = Initialized_Conv1d(in_channels=d_model, out_channels=d_model*2, kernel_size=1, relu=False, bias=False)\n",
    "        self.query_conv = Initialized_Conv1d(in_channels=d_model, out_channels=d_model, kernel_size=1, relu=False, bias=False)\n",
    "\n",
    "        bias = torch.empty(1)\n",
    "        nn.init.constant_(bias, 0)\n",
    "        self.bias = nn.Parameter(bias)\n",
    "\n",
    "    def forward(self, queries, mask):\n",
    "        \"\"\"\n",
    "        queries: B x D x L\n",
    "        \"\"\"\n",
    "        memory = queries\n",
    "\n",
    "        memory = self.mem_conv(memory)  # B x 2D x L\n",
    "        query = self.query_conv(queries)  # B x D x L\n",
    "        memory = memory.transpose(1, 2)  # B x L x 2D\n",
    "        query = query.transpose(1, 2)  # B x L x D\n",
    "        Q = self.split_last_dim(query, self.num_head)  # B x L x H//D x H\n",
    "        K, V = [self.split_last_dim(tensor, self.num_head) for tensor in torch.split(memory, self.d_model, dim=2)]\n",
    "        # split memory into 2 (B x L x D). Key & value then split into\n",
    "        key_depth_per_head = self.d_model // self.num_head\n",
    "\n",
    "        Q *= key_depth_per_head**-0.5\n",
    "        x = self.dot_product_attention(Q, K, V, mask = mask)\n",
    "        return self.combine_last_two_dim(x.permute(0,2,1,3)).transpose(1, 2)\n",
    "\n",
    "    def dot_product_attention(self, q, k ,v, bias = False, mask = None):\n",
    "        \"\"\"dot-product attention.\n",
    "        Args:\n",
    "        q: a Tensor with shape [batch, heads, length_q, depth_k]\n",
    "        k: a Tensor with shape [batch, heads, length_kv, depth_k]\n",
    "        v: a Tensor with shape [batch, heads, length_kv, depth_v]\n",
    "        bias: bias Tensor (see attention_bias())\n",
    "        is_training: a bool of training\n",
    "        scope: an optional string\n",
    "        Returns:\n",
    "        A Tensor.\n",
    "        \"\"\"\n",
    "        logits = torch.matmul(q,k.permute(0,1,3,2))\n",
    "        print(logits)\n",
    "        if bias:\n",
    "            logits += self.bias\n",
    "        if mask is not None:\n",
    "            shapes = [x  if x != None else -1 for x in list(logits.size())]\n",
    "            mask = mask.view(shapes[0], 1, 1, shapes[-1])\n",
    "            logits = mask_logits(logits, mask)\n",
    "\n",
    "        weights = F.softmax(logits, dim=-1)\n",
    "        # dropping out the attention links for each of the heads\n",
    "        weights = F.dropout(weights, p=self.dropout, training=self.training)\n",
    "        return torch.matmul(weights, v)\n",
    "\n",
    "    def split_last_dim(self, x, n):\n",
    "        \"\"\"Reshape x so that the last dimension becomes two dimensions.\n",
    "        The first of these two dimensions is n.\n",
    "        Args:\n",
    "        x: a Tensor with shape [..., m]\n",
    "        n: an integer.\n",
    "        Returns:\n",
    "        a Tensor with shape [..., n, m/n]\n",
    "        \"\"\"\n",
    "        old_shape = list(x.size())  # B x L x D\n",
    "        last = old_shape[-1]  # D\n",
    "        new_shape = old_shape[:-1] + [n] + [last // n if last else None] # (B x L) x H x D//H\n",
    "        ret = x.view(new_shape) # B x L x H x D//n\n",
    "        return ret.permute(0, 2, 1, 3)  # B x H x L x D//n\n",
    "\n",
    "    def combine_last_two_dim(self, x):\n",
    "        \"\"\"Reshape x so that the last two dimension become one.\n",
    "        Args:\n",
    "        x: a Tensor with shape [..., a, b]\n",
    "        Returns:\n",
    "        a Tensor with shape [..., ab]\n",
    "        \"\"\"\n",
    "        old_shape = list(x.size())\n",
    "        a, b = old_shape[-2:]\n",
    "        new_shape = old_shape[:-2] + [a * b if a and b else None]\n",
    "        ret = x.contiguous().view(new_shape)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mask(masks, decode=False):\n",
    "    \"\"\"\n",
    "    :param masks: 0 for pad, 1 for non-pad (batch x seq_len)\n",
    "    :param decode: decoders are Auto-Regressive (can't see future words)\n",
    "    :return: mask: (batch x seq_len x seq_len / batch x 1 x seq_len)\n",
    "    \"\"\"\n",
    "    masks = masks.unsqueeze(-2)  # Pad words should not be zeroed across their whole rows\n",
    "    if decode:\n",
    "        masks = masks & torch.from_numpy(np.tril(np.ones(masks.shape[-1]))).byte()\n",
    "    return masks.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(query, key, value, mask=None, dp=None):\n",
    "    \"\"\"\n",
    "    :param query: Query tensor (batch x heads x seq_len x d_k)\n",
    "    :param key: Key tensor (batch x heads x seq_len x d_k)\n",
    "    :param value: Value tensor (batch x heads x seq_len x d_k)\n",
    "    :param mask: Optional mask, same for all heads (batch x heads x seq_len x seq_len)\n",
    "    :param dp: Dropout layer\n",
    "    :return: output, scores (batch x heads x seq_len x d_k), (batch x heads x seq_len x seq_len)\n",
    "    \"\"\"\n",
    "    logits = torch.matmul(query/(key.shape[-1]**(-.5)), key.transpose(-1, -2))  # THIS IS WRONG!\n",
    "    print(logits)\n",
    "    if mask is not None:\n",
    "        logits = logits.masked_fill(mask == 0, -1e9)  # NOT 1e-9. Softmax(1e-9) is still 1.\n",
    "\n",
    "    scores = F.softmax(logits, dim=-1)\n",
    "\n",
    "    if dp is not None:\n",
    "        scores = dp(scores)\n",
    "    return torch.matmul(scores, value), scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, heads, hidden_size, drop_prob=0.):\n",
    "        \"\"\"\n",
    "        :param heads: Number of attention heads to use\n",
    "        :param hidden_size: Dimension of input/output vectors\n",
    "        :param drop_prob: Dropout rate\n",
    "        \"\"\"\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "\n",
    "        assert hidden_size % heads == 0, \"hidden_size not a multiple of heads\"\n",
    "\n",
    "        self.d_k = hidden_size // heads\n",
    "        self.heads = heads\n",
    "        self.Linears = nn.ModuleList([nn.Linear(hidden_size, hidden_size, bias=False) for _ in range(3)])\n",
    "        for Lin in self.Linears:\n",
    "            nn.init.constant_(Lin.weight, 1.)\n",
    "\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        :param q: Query tensor (batch_size x seq_len x hidden_size)\n",
    "        :param k: Key tensor (batch_size x seq_len x hidden_size)\n",
    "        :param v: Value tensor (batch_size x seq_len x hidden_size)\n",
    "        :param mask: Optional mask (batch_size x seq_len x seq_len)\n",
    "        :return: o: output tensor (batch_size x seq_len x hidden_size)\n",
    "        \"\"\"\n",
    "        batch_size = q.shape[0]\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)  # (batch_size x 1 x seq_len x seq_len)\n",
    "        \n",
    "        # Get the Q, K, V in multiple-heads form after linear layers\n",
    "        q, k, v = [l(x).view(batch_size, -1, self.heads, self.d_k).transpose(1, 2)\n",
    "                   for l, x in zip(self.Linears, (q, k, v))]\n",
    "\n",
    "        o, self.attn = self_attention(q, k, v, mask, self.dropout)  # (batch_size, heads, seq_len, d_k)\n",
    "        \n",
    "        o = o.transpose(1, 2).contiguous().view(batch_size, -1, self.heads*self.d_k)\n",
    "\n",
    "        #return self.Linears[-1](o)  # Some dont use this.\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SA = SelfAttention(6, 2, 0.)  # Depth, Heads\n",
    "MHSA = MultiHeadSelfAttention(2, 6, 0.)  # Heads, Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = torch.ones((3,4,6))  # B x L x D\n",
    "M = torch.cat((torch.ones((3,2)), torch.zeros(3,2)), dim=1)  # B x L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[62.3538, 62.3538, 62.3538, 62.3538],\n",
      "          [62.3538, 62.3538, 62.3538, 62.3538],\n",
      "          [62.3538, 62.3538, 62.3538, 62.3538],\n",
      "          [62.3538, 62.3538, 62.3538, 62.3538]],\n",
      "\n",
      "         [[62.3538, 62.3538, 62.3538, 62.3538],\n",
      "          [62.3538, 62.3538, 62.3538, 62.3538],\n",
      "          [62.3538, 62.3538, 62.3538, 62.3538],\n",
      "          [62.3538, 62.3538, 62.3538, 62.3538]]],\n",
      "\n",
      "\n",
      "        [[[62.3538, 62.3538, 62.3538, 62.3538],\n",
      "          [62.3538, 62.3538, 62.3538, 62.3538],\n",
      "          [62.3538, 62.3538, 62.3538, 62.3538],\n",
      "          [62.3538, 62.3538, 62.3538, 62.3538]],\n",
      "\n",
      "         [[62.3538, 62.3538, 62.3538, 62.3538],\n",
      "          [62.3538, 62.3538, 62.3538, 62.3538],\n",
      "          [62.3538, 62.3538, 62.3538, 62.3538],\n",
      "          [62.3538, 62.3538, 62.3538, 62.3538]]],\n",
      "\n",
      "\n",
      "        [[[62.3538, 62.3538, 62.3538, 62.3538],\n",
      "          [62.3538, 62.3538, 62.3538, 62.3538],\n",
      "          [62.3538, 62.3538, 62.3538, 62.3538],\n",
      "          [62.3538, 62.3538, 62.3538, 62.3538]],\n",
      "\n",
      "         [[62.3538, 62.3538, 62.3538, 62.3538],\n",
      "          [62.3538, 62.3538, 62.3538, 62.3538],\n",
      "          [62.3538, 62.3538, 62.3538, 62.3538],\n",
      "          [62.3538, 62.3538, 62.3538, 62.3538]]]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[6., 6., 6., 6., 6., 6.],\n",
       "         [6., 6., 6., 6., 6., 6.],\n",
       "         [6., 6., 6., 6., 6., 6.],\n",
       "         [6., 6., 6., 6., 6., 6.]],\n",
       "\n",
       "        [[6., 6., 6., 6., 6., 6.],\n",
       "         [6., 6., 6., 6., 6., 6.],\n",
       "         [6., 6., 6., 6., 6., 6.],\n",
       "         [6., 6., 6., 6., 6., 6.]],\n",
       "\n",
       "        [[6., 6., 6., 6., 6., 6.],\n",
       "         [6., 6., 6., 6., 6., 6.],\n",
       "         [6., 6., 6., 6., 6., 6.],\n",
       "         [6., 6., 6., 6., 6., 6.]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SA(Q.transpose(1, 2),M).transpose(1, 2)  # B x L x D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[187.0615, 187.0615, 187.0615, 187.0615],\n",
      "          [187.0615, 187.0615, 187.0615, 187.0615],\n",
      "          [187.0615, 187.0615, 187.0615, 187.0615],\n",
      "          [187.0615, 187.0615, 187.0615, 187.0615]],\n",
      "\n",
      "         [[187.0615, 187.0615, 187.0615, 187.0615],\n",
      "          [187.0615, 187.0615, 187.0615, 187.0615],\n",
      "          [187.0615, 187.0615, 187.0615, 187.0615],\n",
      "          [187.0615, 187.0615, 187.0615, 187.0615]]],\n",
      "\n",
      "\n",
      "        [[[187.0615, 187.0615, 187.0615, 187.0615],\n",
      "          [187.0615, 187.0615, 187.0615, 187.0615],\n",
      "          [187.0615, 187.0615, 187.0615, 187.0615],\n",
      "          [187.0615, 187.0615, 187.0615, 187.0615]],\n",
      "\n",
      "         [[187.0615, 187.0615, 187.0615, 187.0615],\n",
      "          [187.0615, 187.0615, 187.0615, 187.0615],\n",
      "          [187.0615, 187.0615, 187.0615, 187.0615],\n",
      "          [187.0615, 187.0615, 187.0615, 187.0615]]],\n",
      "\n",
      "\n",
      "        [[[187.0615, 187.0615, 187.0615, 187.0615],\n",
      "          [187.0615, 187.0615, 187.0615, 187.0615],\n",
      "          [187.0615, 187.0615, 187.0615, 187.0615],\n",
      "          [187.0615, 187.0615, 187.0615, 187.0615]],\n",
      "\n",
      "         [[187.0615, 187.0615, 187.0615, 187.0615],\n",
      "          [187.0615, 187.0615, 187.0615, 187.0615],\n",
      "          [187.0615, 187.0615, 187.0615, 187.0615],\n",
      "          [187.0615, 187.0615, 187.0615, 187.0615]]]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[6., 6., 6., 6., 6., 6.],\n",
       "         [6., 6., 6., 6., 6., 6.],\n",
       "         [6., 6., 6., 6., 6., 6.],\n",
       "         [6., 6., 6., 6., 6., 6.]],\n",
       "\n",
       "        [[6., 6., 6., 6., 6., 6.],\n",
       "         [6., 6., 6., 6., 6., 6.],\n",
       "         [6., 6., 6., 6., 6., 6.],\n",
       "         [6., 6., 6., 6., 6., 6.]],\n",
       "\n",
       "        [[6., 6., 6., 6., 6., 6.],\n",
       "         [6., 6., 6., 6., 6., 6.],\n",
       "         [6., 6., 6., 6., 6., 6.],\n",
       "         [6., 6., 6., 6., 6., 6.]]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MHSA(Q, Q, Q, make_mask(M))  # B x L x D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As seen above, when the scale is wrong, the softmax outputs are still the same. But this is due to non-random inputs. The scale made the dot-product much larger, making the softmax sharp and peaky."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why divide by sqrt(d_k)? Q.KT = sum(qk) for i = 1, ..., d_k. Assume Q & K are random variables with 0 mean, 1 variance. E(X+Y) = E(X) + E(Y) = 0. Var(X+-Y) = Var(X) + Var(Y) -2Cov(X, Y), where Cov(X, Y) = 0  if independent. Therefore, Var(Q.KT) = sum(Var(qk) for i = 1, ..., d_k) = d_k. std(Q.KT) = sqrt(d_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The \"Convolution\" applied by QANet Pytorch is just the same as linear layers. Their kernel size is 1, which means it's a pointwise (1x1) conv, akin to linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = torch.rand((3,4,6))  # B x L x D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[27.5361, 14.9369, 24.5258, 23.2872],\n",
      "          [14.9369,  8.1025, 13.3040, 12.6321],\n",
      "          [24.5258, 13.3040, 21.8446, 20.7414],\n",
      "          [23.2872, 12.6321, 20.7414, 19.6940]],\n",
      "\n",
      "         [[27.5361, 14.9369, 24.5258, 23.2872],\n",
      "          [14.9369,  8.1025, 13.3040, 12.6321],\n",
      "          [24.5258, 13.3040, 21.8446, 20.7414],\n",
      "          [23.2872, 12.6321, 20.7414, 19.6940]]],\n",
      "\n",
      "\n",
      "        [[[16.9138,  8.8713, 17.3691, 16.5678],\n",
      "          [ 8.8713,  4.6529,  9.1100,  8.6898],\n",
      "          [17.3691,  9.1100, 17.8366, 17.0138],\n",
      "          [16.5678,  8.6898, 17.0138, 16.2289]],\n",
      "\n",
      "         [[16.9138,  8.8713, 17.3691, 16.5678],\n",
      "          [ 8.8713,  4.6529,  9.1100,  8.6898],\n",
      "          [17.3691,  9.1100, 17.8366, 17.0138],\n",
      "          [16.5678,  8.6898, 17.0138, 16.2289]]],\n",
      "\n",
      "\n",
      "        [[[ 7.6209, 12.1175, 16.4530, 12.8870],\n",
      "          [12.1175, 19.2673, 26.1609, 20.4909],\n",
      "          [16.4530, 26.1609, 35.5210, 27.8223],\n",
      "          [12.8870, 20.4909, 27.8223, 21.7922]],\n",
      "\n",
      "         [[ 7.6209, 12.1175, 16.4530, 12.8870],\n",
      "          [12.1175, 19.2673, 26.1609, 20.4909],\n",
      "          [16.4530, 26.1609, 35.5210, 27.8223],\n",
      "          [12.8870, 20.4909, 27.8223, 21.7922]]]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[3.9872, 3.9872, 3.9872, 3.9872, 3.9872, 3.9872],\n",
       "         [3.9853, 3.9853, 3.9853, 3.9853, 3.9853, 3.9853],\n",
       "         [3.9872, 3.9872, 3.9872, 3.9872, 3.9872, 3.9872],\n",
       "         [3.9872, 3.9872, 3.9872, 3.9872, 3.9872, 3.9872]],\n",
       "\n",
       "        [[3.1245, 3.1245, 3.1245, 3.1245, 3.1245, 3.1245],\n",
       "         [3.1034, 3.1034, 3.1034, 3.1034, 3.1034, 3.1034],\n",
       "         [3.1245, 3.1245, 3.1245, 3.1245, 3.1245, 3.1245],\n",
       "         [3.1244, 3.1244, 3.1244, 3.1244, 3.1244, 3.1244]],\n",
       "\n",
       "        [[3.3216, 3.3216, 3.3216, 3.3216, 3.3216, 3.3216],\n",
       "         [3.3343, 3.3343, 3.3343, 3.3343, 3.3343, 3.3343],\n",
       "         [3.3352, 3.3352, 3.3352, 3.3352, 3.3352, 3.3352],\n",
       "         [3.3346, 3.3346, 3.3346, 3.3346, 3.3346, 3.3346]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SA(Q.transpose(1, 2),M).transpose(1, 2)  # B x L x D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 82.6084,  44.8108,  73.5774,  69.8617],\n",
      "          [ 44.8108,  24.3076,  39.9120,  37.8964],\n",
      "          [ 73.5774,  39.9120,  65.5338,  62.2243],\n",
      "          [ 69.8617,  37.8964,  62.2243,  59.0819]],\n",
      "\n",
      "         [[ 82.6084,  44.8108,  73.5774,  69.8617],\n",
      "          [ 44.8108,  24.3076,  39.9120,  37.8964],\n",
      "          [ 73.5774,  39.9120,  65.5338,  62.2243],\n",
      "          [ 69.8617,  37.8964,  62.2243,  59.0819]]],\n",
      "\n",
      "\n",
      "        [[[ 50.7414,  26.6138,  52.1073,  49.7034],\n",
      "          [ 26.6138,  13.9588,  27.3301,  26.0693],\n",
      "          [ 52.1073,  27.3301,  53.5099,  51.0413],\n",
      "          [ 49.7034,  26.0693,  51.0413,  48.6866]],\n",
      "\n",
      "         [[ 50.7414,  26.6138,  52.1073,  49.7034],\n",
      "          [ 26.6138,  13.9588,  27.3301,  26.0693],\n",
      "          [ 52.1073,  27.3301,  53.5099,  51.0413],\n",
      "          [ 49.7034,  26.0693,  51.0413,  48.6866]]],\n",
      "\n",
      "\n",
      "        [[[ 22.8627,  36.3525,  49.3591,  38.6611],\n",
      "          [ 36.3525,  57.8019,  78.4827,  61.4726],\n",
      "          [ 49.3591,  78.4827, 106.5630,  83.4668],\n",
      "          [ 38.6611,  61.4726,  83.4668,  65.3765]],\n",
      "\n",
      "         [[ 22.8627,  36.3525,  49.3591,  38.6611],\n",
      "          [ 36.3525,  57.8019,  78.4827,  61.4726],\n",
      "          [ 49.3591,  78.4827, 106.5630,  83.4668],\n",
      "          [ 38.6611,  61.4726,  83.4668,  65.3765]]]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[3.9872, 3.9872, 3.9872, 3.9872, 3.9872, 3.9872],\n",
       "         [3.9872, 3.9872, 3.9872, 3.9872, 3.9872, 3.9872],\n",
       "         [3.9872, 3.9872, 3.9872, 3.9872, 3.9872, 3.9872],\n",
       "         [3.9872, 3.9872, 3.9872, 3.9872, 3.9872, 3.9872]],\n",
       "\n",
       "        [[3.1249, 3.1249, 3.1249, 3.1249, 3.1249, 3.1249],\n",
       "         [3.1249, 3.1249, 3.1249, 3.1249, 3.1249, 3.1249],\n",
       "         [3.1249, 3.1249, 3.1249, 3.1249, 3.1249, 3.1249],\n",
       "         [3.1249, 3.1249, 3.1249, 3.1249, 3.1249, 3.1249]],\n",
       "\n",
       "        [[3.3353, 3.3353, 3.3353, 3.3353, 3.3353, 3.3353],\n",
       "         [3.3353, 3.3353, 3.3353, 3.3353, 3.3353, 3.3353],\n",
       "         [3.3353, 3.3353, 3.3353, 3.3353, 3.3353, 3.3353],\n",
       "         [3.3353, 3.3353, 3.3353, 3.3353, 3.3353, 3.3353]]],\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MHSA(Q, Q, Q, make_mask(M))  # B x L x D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As seen above, when random inputs are fed, the wrongly-scaled attention has similar softmax outputs since the inputs have all been scaled very large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
