{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math,copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "#     print(math.sqrt(d_k))\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    print(torch.matmul(p_attn, value))\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(query, key, value, mask=None, dp=None):\n",
    "    \"\"\"\n",
    "    :param query: Query tensor (batch x heads x seq_len x d_k)\n",
    "    :param key: Key tensor (batch x heads x seq_len x d_k)\n",
    "    :param value: Value tensor (batch x heads x seq_len x d_k)\n",
    "    :param mask: Optional mask, same for all heads (batch x heads x seq_len x seq_len)\n",
    "    :param dp: Dropout layer\n",
    "    :return: output, scores (batch x heads x seq_len x d_k), (batch x heads x seq_len x seq_len)\n",
    "    \"\"\"\n",
    "#     print(math.sqrt(key.shape[-1]))\n",
    "    logits = torch.matmul(query, key.transpose(-2, -1))/math.sqrt(key.shape[-1])\n",
    "\n",
    "    if mask is not None:\n",
    "        logits = logits.masked_fill(mask==0, -1e9)  # NOT 1e-9. Softmax(1e-9) is still 1.\n",
    "    scores = F.softmax(logits, dim=-1)\n",
    "\n",
    "    if dp is not None:\n",
    "        scores = dp(scores)\n",
    "    print(torch.matmul(scores, value))\n",
    "    return torch.matmul(scores, value), scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.0):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        for p in self.parameters():\n",
    "            torch.nn.init.constant_(p, 1.0)\n",
    "\n",
    "            \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "\n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, heads, hidden_size, drop_prob=0.):\n",
    "        \"\"\"\n",
    "        :param heads: Number of attention heads to use\n",
    "        :param hidden_size: Dimension of input/output vectors\n",
    "        :param drop_prob: Dropout rate\n",
    "        \"\"\"\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "\n",
    "        assert hidden_size % heads == 0, \"hidden_size not a multiple of heads\"\n",
    "\n",
    "        self.d_k = hidden_size // heads\n",
    "        self.heads = heads\n",
    "        self.Linears = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(4)])\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "        for p in self.parameters():\n",
    "            torch.nn.init.constant_(p, 1.0)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        :param q: Query tensor (batch_size x seq_len x hidden_size)\n",
    "        :param k: Key tensor (batch_size x seq_len x hidden_size)\n",
    "        :param v: Value tensor (batch_size x seq_len x hidden_size)\n",
    "        :param mask: Optional mask (batch_size x seq_len x seq_len)\n",
    "        :return: o: output tensor (batch_size x seq_len x hidden_size)\n",
    "        \"\"\"\n",
    "        batch_size = q.shape[0]\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)  # (batch_size x 1 x seq_len x seq_len)\n",
    "\n",
    "        # Get the Q, K, V in multiple-heads form after linear layers\n",
    "        q, k, v = [l(x).view(batch_size, -1, self.heads, self.d_k).transpose(1, 2)\n",
    "                   for l, x in zip(self.Linears, (q, k, v))]\n",
    "\n",
    "        o, self.attn = self_attention(q, k, v, mask, self.dropout)  # (batch_size, heads, seq_len, d_k)\n",
    "\n",
    "        o = o.transpose(1, 2).contiguous().view(batch_size, -1, self.heads*self.d_k)\n",
    "\n",
    "        return self.Linears[-1](o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadedAttention(\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (1): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (2): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (3): Linear(in_features=12, out_features=12, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "MHA=MultiHeadedAttention(4,12)\n",
    "print(MHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadSelfAttention(\n",
      "  (Linears): ModuleList(\n",
      "    (0): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (1): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (2): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (3): Linear(in_features=12, out_features=12, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "MHSA=MultiHeadSelfAttention(4,12)\n",
    "print(MHSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((1,4,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 2.4557,  2.4557,  2.4557],\n",
      "          [ 2.5077,  2.5077,  2.5077],\n",
      "          [-6.6023, -6.6023, -6.6023],\n",
      "          [ 2.5013,  2.5013,  2.5013]],\n",
      "\n",
      "         [[ 2.4557,  2.4557,  2.4557],\n",
      "          [ 2.5077,  2.5077,  2.5077],\n",
      "          [-6.6023, -6.6023, -6.6023],\n",
      "          [ 2.5013,  2.5013,  2.5013]],\n",
      "\n",
      "         [[ 2.4557,  2.4557,  2.4557],\n",
      "          [ 2.5077,  2.5077,  2.5077],\n",
      "          [-6.6023, -6.6023, -6.6023],\n",
      "          [ 2.5013,  2.5013,  2.5013]],\n",
      "\n",
      "         [[ 2.4557,  2.4557,  2.4557],\n",
      "          [ 2.5077,  2.5077,  2.5077],\n",
      "          [-6.6023, -6.6023, -6.6023],\n",
      "          [ 2.5013,  2.5013,  2.5013]]]], grad_fn=<UnsafeViewBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 30.4684,  30.4684,  30.4684,  30.4684,  30.4684,  30.4684,  30.4684,\n",
       "           30.4684,  30.4684,  30.4684,  30.4684,  30.4684],\n",
       "         [ 31.0919,  31.0919,  31.0919,  31.0919,  31.0919,  31.0919,  31.0919,\n",
       "           31.0919,  31.0919,  31.0919,  31.0919,  31.0919],\n",
       "         [-78.2277, -78.2277, -78.2277, -78.2277, -78.2277, -78.2277, -78.2277,\n",
       "          -78.2277, -78.2277, -78.2277, -78.2277, -78.2277],\n",
       "         [ 31.0157,  31.0157,  31.0157,  31.0157,  31.0157,  31.0157,  31.0157,\n",
       "           31.0157,  31.0157,  31.0157,  31.0157,  31.0157]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MHSA(x,x,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 2.4557,  2.4557,  2.4557],\n",
      "          [ 2.5077,  2.5077,  2.5077],\n",
      "          [-6.6023, -6.6023, -6.6023],\n",
      "          [ 2.5013,  2.5013,  2.5013]],\n",
      "\n",
      "         [[ 2.4557,  2.4557,  2.4557],\n",
      "          [ 2.5077,  2.5077,  2.5077],\n",
      "          [-6.6023, -6.6023, -6.6023],\n",
      "          [ 2.5013,  2.5013,  2.5013]],\n",
      "\n",
      "         [[ 2.4557,  2.4557,  2.4557],\n",
      "          [ 2.5077,  2.5077,  2.5077],\n",
      "          [-6.6023, -6.6023, -6.6023],\n",
      "          [ 2.5013,  2.5013,  2.5013]],\n",
      "\n",
      "         [[ 2.4557,  2.4557,  2.4557],\n",
      "          [ 2.5077,  2.5077,  2.5077],\n",
      "          [-6.6023, -6.6023, -6.6023],\n",
      "          [ 2.5013,  2.5013,  2.5013]]]], grad_fn=<UnsafeViewBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 30.4684,  30.4684,  30.4684,  30.4684,  30.4684,  30.4684,  30.4684,\n",
       "           30.4684,  30.4684,  30.4684,  30.4684,  30.4684],\n",
       "         [ 31.0919,  31.0919,  31.0919,  31.0919,  31.0919,  31.0919,  31.0919,\n",
       "           31.0919,  31.0919,  31.0919,  31.0919,  31.0919],\n",
       "         [-78.2277, -78.2277, -78.2277, -78.2277, -78.2277, -78.2277, -78.2277,\n",
       "          -78.2277, -78.2277, -78.2277, -78.2277, -78.2277],\n",
       "         [ 31.0157,  31.0157,  31.0157,  31.0157,  31.0157,  31.0157,  31.0157,\n",
       "           31.0157,  31.0157,  31.0157,  31.0157,  31.0157]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MHA(x,x,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1588, -0.7460,  0.2738,  0.3562, -0.4570,  0.5218,  0.1263,\n",
      "           0.4898,  1.0624, -0.3421,  2.0724,  0.5900],\n",
      "         [-0.3360, -1.6072, -0.8374,  1.1898, -1.4670,  0.6100,  0.5050,\n",
      "          -0.6781,  1.6863, -1.1524,  1.3355,  1.5912],\n",
      "         [ 1.5464, -0.5643,  2.1567,  0.7049,  0.4197,  1.7633,  0.7826,\n",
      "           0.7569,  2.5032,  0.6617,  0.0387, -1.2237],\n",
      "         [-0.8238, -0.6416,  1.5324,  1.8258, -1.4826,  2.1162, -0.7381,\n",
      "          -0.4709, -0.6954,  0.4675,  1.2878, -0.1611]]])\n"
     ]
    }
   ],
   "source": [
    "o1 = self_attention(x, x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1588, -0.7460,  0.2738,  0.3562, -0.4570,  0.5218,  0.1263,\n",
      "           0.4898,  1.0624, -0.3421,  2.0724,  0.5900],\n",
      "         [-0.3360, -1.6072, -0.8374,  1.1898, -1.4670,  0.6100,  0.5050,\n",
      "          -0.6781,  1.6863, -1.1524,  1.3355,  1.5912],\n",
      "         [ 1.5464, -0.5643,  2.1567,  0.7049,  0.4197,  1.7633,  0.7826,\n",
      "           0.7569,  2.5032,  0.6617,  0.0387, -1.2237],\n",
      "         [-0.8238, -0.6416,  1.5324,  1.8258, -1.4826,  2.1162, -0.7381,\n",
      "          -0.4709, -0.6954,  0.4675,  1.2878, -0.1611]]])\n"
     ]
    }
   ],
   "source": [
    "o2 = attention(x, x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
