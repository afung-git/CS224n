{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = layers.TransformerEncoderStack(\n",
    "                N=7,\n",
    "                heads=8,\n",
    "                input_size=96,\n",
    "                output_size=96,\n",
    "                inter_size=96*4,\n",
    "                num_conv=2,\n",
    "                drop_prob=.1\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoderStack(\n",
       "  (layers): ModuleList(\n",
       "    (0): TransformerEncoder(\n",
       "      (PE): PositionalEncodings(\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (convs): ModuleList(\n",
       "        (0): InitializedLayer(\n",
       "          (ff): Sequential(\n",
       "            (0): Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(3,), groups=96, bias=False)\n",
       "            (1): Conv1d(96, 96, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (1): InitializedLayer(\n",
       "          (ff): Sequential(\n",
       "            (0): Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(3,), groups=96, bias=False)\n",
       "            (1): Conv1d(96, 96, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (MHSA): MultiHeadSelfAttention(\n",
       "        (Linears): ModuleList(\n",
       "          (0): InitializedLayer(\n",
       "            (ff): Linear(in_features=96, out_features=96, bias=False)\n",
       "          )\n",
       "          (1): InitializedLayer(\n",
       "            (ff): Linear(in_features=96, out_features=96, bias=False)\n",
       "          )\n",
       "          (2): InitializedLayer(\n",
       "            (ff): Linear(in_features=96, out_features=96, bias=False)\n",
       "          )\n",
       "          (3): InitializedLayer(\n",
       "            (ff): Linear(in_features=96, out_features=96, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (FF): FeedForward(\n",
       "        (FF): Sequential(\n",
       "          (0): InitializedLayer(\n",
       "            (ff): Linear(in_features=96, out_features=384, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.1)\n",
       "          (2): InitializedLayer(\n",
       "            (ff): Linear(in_features=384, out_features=96, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0): Sublayer(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): Sublayer(\n",
       "          (dropout): Dropout(p=0.0)\n",
       "          (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): Sublayer(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): Sublayer(\n",
       "          (dropout): Dropout(p=0.0)\n",
       "          (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerEncoder(\n",
       "      (PE): PositionalEncodings(\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (convs): ModuleList(\n",
       "        (0): InitializedLayer(\n",
       "          (ff): Sequential(\n",
       "            (0): Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(3,), groups=96, bias=False)\n",
       "            (1): Conv1d(96, 96, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (1): InitializedLayer(\n",
       "          (ff): Sequential(\n",
       "            (0): Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(3,), groups=96, bias=False)\n",
       "            (1): Conv1d(96, 96, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (MHSA): MultiHeadSelfAttention(\n",
       "        (Linears): ModuleList(\n",
       "          (0): InitializedLayer(\n",
       "            (ff): Linear(in_features=96, out_features=96, bias=False)\n",
       "          )\n",
       "          (1): InitializedLayer(\n",
       "            (ff): Linear(in_features=96, out_features=96, bias=False)\n",
       "          )\n",
       "          (2): InitializedLayer(\n",
       "            (ff): Linear(in_features=96, out_features=96, bias=False)\n",
       "          )\n",
       "          (3): InitializedLayer(\n",
       "            (ff): Linear(in_features=96, out_features=96, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (FF): FeedForward(\n",
       "        (FF): Sequential(\n",
       "          (0): InitializedLayer(\n",
       "            (ff): Linear(in_features=96, out_features=384, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.1)\n",
       "          (2): InitializedLayer(\n",
       "            (ff): Linear(in_features=384, out_features=96, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0): Sublayer(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): Sublayer(\n",
       "          (dropout): Dropout(p=0.0)\n",
       "          (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): Sublayer(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): Sublayer(\n",
       "          (dropout): Dropout(p=0.0)\n",
       "          (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerEncoder(\n",
       "      (PE): PositionalEncodings(\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (convs): ModuleList(\n",
       "        (0): InitializedLayer(\n",
       "          (ff): Sequential(\n",
       "            (0): Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(3,), groups=96, bias=False)\n",
       "            (1): Conv1d(96, 96, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (1): InitializedLayer(\n",
       "          (ff): Sequential(\n",
       "            (0): Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(3,), groups=96, bias=False)\n",
       "            (1): Conv1d(96, 96, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (MHSA): MultiHeadSelfAttention(\n",
       "        (Linears): ModuleList(\n",
       "          (0): InitializedLayer(\n",
       "            (ff): Linear(in_features=96, out_features=96, bias=False)\n",
       "          )\n",
       "          (1): InitializedLayer(\n",
       "            (ff): Linear(in_features=96, out_features=96, bias=False)\n",
       "          )\n",
       "          (2): InitializedLayer(\n",
       "            (ff): Linear(in_features=96, out_features=96, bias=False)\n",
       "          )\n",
       "          (3): InitializedLayer(\n",
       "            (ff): Linear(in_features=96, out_features=96, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (FF): FeedForward(\n",
       "        (FF): Sequential(\n",
       "          (0): InitializedLayer(\n",
       "            (ff): Linear(in_features=96, out_features=384, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.1)\n",
       "          (2): InitializedLayer(\n",
       "            (ff): Linear(in_features=384, out_features=96, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0): Sublayer(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): Sublayer(\n",
       "          (dropout): Dropout(p=0.0)\n",
       "          (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): Sublayer(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): Sublayer(\n",
       "          (dropout): Dropout(p=0.0)\n",
       "          (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerEncoder(\n",
       "      (PE): PositionalEncodings(\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (convs): ModuleList(\n",
       "        (0): InitializedLayer(\n",
       "          (ff): Sequential(\n",
       "            (0): Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(3,), groups=96, bias=False)\n",
       "            (1): Conv1d(96, 96, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (1): InitializedLayer(\n",
       "          (ff): Sequential(\n",
       "            (0): Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(3,), groups=96, bias=False)\n",
       "            (1): Conv1d(96, 96, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (MHSA): MultiHeadSelfAttention(\n",
       "        (Linears): ModuleList(\n",
       "          (0): InitializedLayer(\n",
       "            (ff): Linear(in_features=96, out_features=96, bias=False)\n",
       "          )\n",
       "          (1): InitializedLayer(\n",
       "            (ff): Linear(in_features=96, out_features=96, bias=False)\n",
       "          )\n",
       "          (2): InitializedLayer(\n",
       "            (ff): Linear(in_features=96, out_features=96, bias=False)\n",
       "          )\n",
       "          (3): InitializedLayer(\n",
       "            (ff): Linear(in_features=96, out_features=96, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (FF): FeedForward(\n",
       "        (FF): Sequential(\n",
       "          (0): InitializedLayer(\n",
       "            (ff): Linear(in_features=96, out_features=384, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.1)\n",
       "          (2): InitializedLayer(\n",
       "            (ff): Linear(in_features=384, out_features=96, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0): Sublayer(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): Sublayer(\n",
       "          (dropout): Dropout(p=0.0)\n",
       "          (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): Sublayer(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): Sublayer(\n",
       "          (dropout): Dropout(p=0.0)\n",
       "          (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerEncoder(\n",
       "      (PE): PositionalEncodings(\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (convs): ModuleList(\n",
       "        (0): InitializedLayer(\n",
       "          (ff): Sequential(\n",
       "            (0): Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(3,), groups=96, bias=False)\n",
       "            (1): Conv1d(96, 96, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (1): InitializedLayer(\n",
       "          (ff): Sequential(\n",
       "            (0): Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(3,), groups=96, bias=False)\n",
       "            (1): Conv1d(96, 96, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (MHSA): MultiHeadSelfAttention(\n",
       "        (Linears): ModuleList(\n",
       "          (0): InitializedLayer(\n",
       "            (ff): Linear(in_features=96, out_features=96, bias=False)\n",
       "          )\n",
       "          (1): InitializedLayer(\n",
       "            (ff): Linear(in_features=96, out_features=96, bias=False)\n",
       "          )\n",
       "          (2): InitializedLayer(\n",
       "            (ff): Linear(in_features=96, out_features=96, bias=False)\n",
       "          )\n",
       "          (3): InitializedLayer(\n",
       "            (ff): Linear(in_features=96, out_features=96, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (FF): FeedForward(\n",
       "        (FF): Sequential(\n",
       "          (0): InitializedLayer(\n",
       "            (ff): Linear(in_features=96, out_features=384, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.1)\n",
       "          (2): InitializedLayer(\n",
       "            (ff): Linear(in_features=384, out_features=96, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0): Sublayer(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): Sublayer(\n",
       "          (dropout): Dropout(p=0.0)\n",
       "          (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): Sublayer(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): Sublayer(\n",
       "          (dropout): Dropout(p=0.0)\n",
       "          (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerEncoder(\n",
       "      (PE): PositionalEncodings(\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (convs): ModuleList(\n",
       "        (0): InitializedLayer(\n",
       "          (ff): Sequential(\n",
       "            (0): Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(3,), groups=96, bias=False)\n",
       "            (1): Conv1d(96, 96, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (1): InitializedLayer(\n",
       "          (ff): Sequential(\n",
       "            (0): Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(3,), groups=96, bias=False)\n",
       "            (1): Conv1d(96, 96, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (MHSA): MultiHeadSelfAttention(\n",
       "        (Linears): ModuleList(\n",
       "          (0): InitializedLayer(\n",
       "            (ff): Linear(in_features=96, out_features=96, bias=False)\n",
       "          )\n",
       "          (1): InitializedLayer(\n",
       "            (ff): Linear(in_features=96, out_features=96, bias=False)\n",
       "          )\n",
       "          (2): InitializedLayer(\n",
       "            (ff): Linear(in_features=96, out_features=96, bias=False)\n",
       "          )\n",
       "          (3): InitializedLayer(\n",
       "            (ff): Linear(in_features=96, out_features=96, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (FF): FeedForward(\n",
       "        (FF): Sequential(\n",
       "          (0): InitializedLayer(\n",
       "            (ff): Linear(in_features=96, out_features=384, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.1)\n",
       "          (2): InitializedLayer(\n",
       "            (ff): Linear(in_features=384, out_features=96, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0): Sublayer(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): Sublayer(\n",
       "          (dropout): Dropout(p=0.0)\n",
       "          (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): Sublayer(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): Sublayer(\n",
       "          (dropout): Dropout(p=0.0)\n",
       "          (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (last): TransformerEncoder(\n",
       "    (PE): PositionalEncodings(\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (convs): ModuleList(\n",
       "      (0): InitializedLayer(\n",
       "        (ff): Sequential(\n",
       "          (0): Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(3,), groups=96, bias=False)\n",
       "          (1): Conv1d(96, 96, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (1): InitializedLayer(\n",
       "        (ff): Sequential(\n",
       "          (0): Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(3,), groups=96, bias=False)\n",
       "          (1): Conv1d(96, 96, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (MHSA): MultiHeadSelfAttention(\n",
       "      (Linears): ModuleList(\n",
       "        (0): InitializedLayer(\n",
       "          (ff): Linear(in_features=96, out_features=96, bias=False)\n",
       "        )\n",
       "        (1): InitializedLayer(\n",
       "          (ff): Linear(in_features=96, out_features=96, bias=False)\n",
       "        )\n",
       "        (2): InitializedLayer(\n",
       "          (ff): Linear(in_features=96, out_features=96, bias=False)\n",
       "        )\n",
       "        (3): InitializedLayer(\n",
       "          (ff): Linear(in_features=96, out_features=96, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (FF): FeedForward(\n",
       "      (FF): Sequential(\n",
       "        (0): InitializedLayer(\n",
       "          (ff): Linear(in_features=96, out_features=384, bias=True)\n",
       "        )\n",
       "        (1): Dropout(p=0.1)\n",
       "        (2): InitializedLayer(\n",
       "          (ff): Linear(in_features=384, out_features=96, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): Sublayer(\n",
       "        (dropout): Dropout(p=0.1)\n",
       "        (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): Sublayer(\n",
       "        (dropout): Dropout(p=0.0)\n",
       "        (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): Sublayer(\n",
       "        (dropout): Dropout(p=0.1)\n",
       "        (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): Sublayer(\n",
       "        (dropout): Dropout(p=0.0)\n",
       "        (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = layers.TransformerEncoderStack(\n",
    "                N=1,\n",
    "                heads=8,\n",
    "                input_size=96,\n",
    "                output_size=96,\n",
    "                inter_size=96*4,\n",
    "                num_conv=4,\n",
    "                drop_prob=.1\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoderStack(\n",
       "  (layers): ModuleList()\n",
       "  (last): TransformerEncoder(\n",
       "    (PE): PositionalEncodings(\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (convs): ModuleList(\n",
       "      (0): InitializedLayer(\n",
       "        (ff): Sequential(\n",
       "          (0): Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(3,), groups=96, bias=False)\n",
       "          (1): Conv1d(96, 96, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (1): InitializedLayer(\n",
       "        (ff): Sequential(\n",
       "          (0): Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(3,), groups=96, bias=False)\n",
       "          (1): Conv1d(96, 96, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): InitializedLayer(\n",
       "        (ff): Sequential(\n",
       "          (0): Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(3,), groups=96, bias=False)\n",
       "          (1): Conv1d(96, 96, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (3): InitializedLayer(\n",
       "        (ff): Sequential(\n",
       "          (0): Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(3,), groups=96, bias=False)\n",
       "          (1): Conv1d(96, 96, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (MHSA): MultiHeadSelfAttention(\n",
       "      (Linears): ModuleList(\n",
       "        (0): InitializedLayer(\n",
       "          (ff): Linear(in_features=96, out_features=96, bias=False)\n",
       "        )\n",
       "        (1): InitializedLayer(\n",
       "          (ff): Linear(in_features=96, out_features=96, bias=False)\n",
       "        )\n",
       "        (2): InitializedLayer(\n",
       "          (ff): Linear(in_features=96, out_features=96, bias=False)\n",
       "        )\n",
       "        (3): InitializedLayer(\n",
       "          (ff): Linear(in_features=96, out_features=96, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (FF): FeedForward(\n",
       "      (FF): Sequential(\n",
       "        (0): InitializedLayer(\n",
       "          (ff): Linear(in_features=96, out_features=384, bias=True)\n",
       "        )\n",
       "        (1): Dropout(p=0.1)\n",
       "        (2): InitializedLayer(\n",
       "          (ff): Linear(in_features=384, out_features=96, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): Sublayer(\n",
       "        (dropout): Dropout(p=0.1)\n",
       "        (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): Sublayer(\n",
       "        (dropout): Dropout(p=0.0)\n",
       "        (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): Sublayer(\n",
       "        (dropout): Dropout(p=0.1)\n",
       "        (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): Sublayer(\n",
       "        (dropout): Dropout(p=0.0)\n",
       "        (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): Sublayer(\n",
       "        (dropout): Dropout(p=0.1)\n",
       "        (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): Sublayer(\n",
       "        (dropout): Dropout(p=0.0)\n",
       "        (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm(torch.Size([96]), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.layers_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[28]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod.layers_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[28]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod.last.layers_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[28], [28], [28], [28], [28], [28]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[l.layers_count for l in mod.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ll.id for l in mod.layers for ll in l.layers ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
